{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4237cd",
   "metadata": {},
   "source": [
    "# PHISHES Digital Platform - Data Download Module\n",
    "\n",
    "This notebook downloads and analyzes geospatial datasets from Azure Blob Storage for catchment-scale soil science modeling.\n",
    "\n",
    "## Workflow Overview\n",
    "1. **Step 1: Setup and Imports**: Import libraries and configure logging\n",
    "2. **Step 2: Configure Paths**: Define catchment shapefile and project settings\n",
    "3. **Step 3: Initialize**: Create project structure and setup downloader\n",
    "4. **Step 4: Inspect Catchment**: Load and visualize study area boundary\n",
    "5. **Step 5: Initialize Downloader**: Connect to Azure and configure the downloader\n",
    "6. **Step 6: Browse Datasets**: List available categories and subcategories\n",
    "7. **Step 7: Download Data**: Fetch climate, soil, or topography datasets clipped to catchment\n",
    "8. **Step 8: Visualize**: Create spatial maps and temporal plots\n",
    "9. **Step 9: Analyze**: Compute basin averages and time series statistics\n",
    "10. **Step 10: Review Logs**: Inspect download history and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3e603",
   "metadata": {},
   "source": [
    "## üìã Step 1: Setup and Imports\n",
    "\n",
    "**Imports all required libraries and configures logging.**\n",
    "\n",
    "This cell:\n",
    "- Imports core functionality (folder creation, downloader, data loading)\n",
    "- Imports analysis functions (catchment processing, basin averaging)\n",
    "- Imports visualization tools (spatial maps, time series plots)\n",
    "- Configures logging to track download progress and errors\n",
    "- Suppresses unnecessary warnings\n",
    "\n",
    "‚úÖ Run this cell first before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.analysis import (\n",
    "    compute_anomalies,\n",
    "    compute_basin_average,\n",
    "    compute_catchment_weights,\n",
    "    load_catchment,\n",
    "    plot_catchment,\n",
    "    plot_spatial_map,\n",
    "    plot_time_series,\n",
    ")\n",
    "from src.core import (\n",
    "    PDPDataDownloader,\n",
    "    build_dataset_path,\n",
    "    cleanup_existing_dataset,\n",
    "    create_pdp_folders,\n",
    "    open_dataset_any,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad853f2d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Configuration\n",
    "\n",
    "‚úÖ **Edit the paths below to match your setup.**\n",
    "\n",
    "This cell defines:\n",
    "- Project name and base directory for downloaded data\n",
    "- Catchment file path (Shapefile `.shp` or GeoJSON `.geojson`)\n",
    "- Time range for temporal datasets (e.g., climate data)\n",
    "- Output format preference (NetCDF, Zarr, or DFS2)\n",
    "- Whether to mask data to the exact catchment shape (`True`) or keep the full bounding-box extent (`False`)\n",
    "\n",
    "Editable parameters:\n",
    "- `PROJECT_MAIN`: Short project identifier used in folder names\n",
    "- `PROJECT_NAME`: Final project name (can be the same as `PROJECT_MAIN`)\n",
    "- `PROJECT_BASE`: Base output directory for downloads. Default: sub-directory `data`\n",
    "- `CATCHMENT_FILE`: Path to your catchment file (`.shp` or `.geojson`)\n",
    "- `MASK_ON_CATCHMENT`: `True` to mask to catchment boundary, `False` to keep bounding box\n",
    "- `CATCHMENT_EXTENT`: Optional manual extent `[minx, miny, maxx, maxy]`\n",
    "- `CATCHMENT_CRS`: CRS for the manual extent (e.g., `EPSG:4326`)\n",
    "- `TIME_RANGE`: Optional time range tuple `(start_date, end_date)`\n",
    "- `OUTPUT_FORMAT`: Output format (`nc`, `zarr`, or `dfs2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "PROJECT_MAIN = \"\"  # str: short project identifier, used in folder names\n",
    "PROJECT_NAME = f\"{PROJECT_MAIN}\"  # str: final project name (can be same as PROJECT_MAIN)\n",
    "PROJECT_BASE = Path(f\"../data/{PROJECT_NAME}\")  # Path: base output directory for downloads\n",
    "\n",
    "# Path to your catchment file (Shapefile .shp or GeoJSON .geojson)\n",
    "CATCHMENT_FILE = Path(r\"\")\n",
    "MASK_ON_CATCHMENT = True  # Whether to mask datasets to the catchment boundary (True/False)\n",
    "\n",
    "# Optional manual extent (use instead of CATCHMENT_FILE)\n",
    "CATCHMENT_EXTENT = None  # [minx, miny, maxx, maxy]\n",
    "CATCHMENT_CRS = \"EPSG:4326\"  # CRS for the extent\n",
    "\n",
    "# Time range for temporal datasets (optional)\n",
    "TIME_RANGE = (\"2015-01-01\", \"2020-12-31\")  # Adjust as needed\n",
    "\n",
    "# Output format (\"nc\", \"zarr\", or \"dfs2\")\n",
    "OUTPUT_FORMAT = \"dfs2\"\n",
    "\n",
    "print(f\"Project base:   {PROJECT_BASE}\")\n",
    "print(f\"Catchment:      {CATCHMENT_FILE}\")\n",
    "print(f\"Mask dataset:   {MASK_ON_CATCHMENT}\")\n",
    "print(f\"Time range:     {TIME_RANGE}\")\n",
    "print(f\"Output format:  {OUTPUT_FORMAT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf84200",
   "metadata": {},
   "source": [
    "## üîß Step 3: Initialize Project Structure\n",
    "\n",
    "**Creates the project folder hierarchy for organized data management.**\n",
    "\n",
    "This cell:\n",
    "- Creates base project directory if it doesn't exist\n",
    "- Creates `logs/` subdirectory for download tracking\n",
    "- Displays confirmation of project initialization\n",
    "- Data folders (climate/, soil/, etc.) are created automatically during downloads\n",
    "\n",
    "### Initial structure\n",
    "\n",
    "```\n",
    "<your_project>/\n",
    "‚îî‚îÄ‚îÄ logs/\n",
    "    ‚îî‚îÄ‚îÄ download_log.json\n",
    "```\n",
    "\n",
    "### After downloading datasets\n",
    "\n",
    "```\n",
    "<your_project>/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ <category>/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ <subcategory>/\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ <subcategory>.<format>\n",
    "‚îî‚îÄ‚îÄ logs/\n",
    "    ‚îî‚îÄ‚îÄ download_log.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project (creates base folder and logs directory)\n",
    "print(\"Initializing PDP project...\")\n",
    "base_path = create_pdp_folders(base_path=PROJECT_BASE)\n",
    "\n",
    "print(f\"Project initialized at: {base_path}\")\n",
    "print(\"Data folders will be created automatically when downloading datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3fe8f",
   "metadata": {},
   "source": [
    "## üìç Step 4: Load and Visualize Catchment\n",
    "\n",
    "**Read catchment shapefile and display study area boundary.**\n",
    "\n",
    "This cell:\n",
    "- Loads catchment polygon from shapefile (loaded once, reused throughout)\n",
    "- Supports **polygons**, **lines**, and **points** (lines/points are buffered to polygons)\n",
    "- Automatically reprojects to EPSG:4326 (lat/lon) if needed\n",
    "- Merges multiple features into single polygon if present\n",
    "- **Validates European AOI**: Checks catchment overlaps with Europe\n",
    "- **Validates size**: Ensures area is within 0.01 - 500,000 km¬≤\n",
    "- Displays catchment bounds (min/max lat/lon)\n",
    "- Creates visualization map of catchment boundary\n",
    "\n",
    "**Alternative:** You can use a manual extent and projection (bbox + CRS) instead of a polygon file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ca157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize catchment (loaded once here, reused throughout notebook)\n",
    "# Supports polygons, lines, and points (lines/points are buffered to polygons)\n",
    "# Validates: European AOI overlap, catchment size limits\n",
    "\n",
    "if CATCHMENT_EXTENT is not None:\n",
    "    catchment, catchment_geom = load_catchment(\n",
    "        extent=CATCHMENT_EXTENT,\n",
    "        extent_crs=CATCHMENT_CRS,\n",
    "        target_crs=\"EPSG:4326\",\n",
    "    )\n",
    "else:\n",
    "    catchment, catchment_geom = load_catchment(CATCHMENT_FILE, target_crs=\"EPSG:4326\")\n",
    "\n",
    "print(f\"Catchment bounds: {catchment.total_bounds}\")\n",
    "\n",
    "fig, ax = plot_catchment(catchment, title=\"Catchment Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1fbaed",
   "metadata": {},
   "source": [
    "## üåê Step 5: Initialize Data Downloader\n",
    "\n",
    "**Setup Azure connection and configure downloader with catchment.**\n",
    "\n",
    "This cell:\n",
    "- Uses the catchment geometry loaded in Step 4 (no redundant file read)\n",
    "- Initializes connection to Azure Blob Storage using credentials\n",
    "- Sets buffer distance (in grid cells) around catchment (default: 1 cell)\n",
    "- Specifies output format (NetCDF or Zarr)\n",
    "- Tests connection and displays any errors with troubleshooting tips\n",
    "\n",
    "Editable parameters:\n",
    "- `catchment`: GeoDataFrame loaded in Step 4\n",
    "- `output_base`: Base output directory (uses `PROJECT_BASE`)\n",
    "- `buffer_cells`: Number of grid cells to buffer around the catchment\n",
    "- `output_format`: Output format (uses `OUTPUT_FORMAT`)\n",
    "- `mask_on_catchment`: `True` to mask to catchment boundary, `False` to keep bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize downloader (uses catchment loaded in Step 3)\n",
    "print(\"Initializing data downloader...\")\n",
    "\n",
    "try:\n",
    "    downloader = PDPDataDownloader(\n",
    "        catchment=catchment,  # Use pre-loaded GeoDataFrame from Step 3\n",
    "        output_base=PROJECT_BASE,\n",
    "        buffer_cells=1,  # Buffer in grid cells\n",
    "        output_format=OUTPUT_FORMAT,\n",
    "        mask_on_catchment=MASK_ON_CATCHMENT,\n",
    "    )\n",
    "    print(\"Downloader initialized successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize downloader: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. Catchment was loaded in Step 3\")\n",
    "    print(\"2. Azure credentials are configured\")\n",
    "    print(\"3. You have internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45941df5",
   "metadata": {},
   "source": [
    "## üìö Step 6: Browse Available Datasets\n",
    "\n",
    "**Query Azure storage to see what datasets are available for download.**\n",
    "\n",
    "This cell:\n",
    "- Queries the dataset catalog from Azure storage\n",
    "- Lists available categories (climate, soil, topography, landuse, hydrology)\n",
    "- Displays subcategories and their descriptions\n",
    "- Shows available variables for each dataset\n",
    "- Helps you decide what to download\n",
    "\n",
    "Editable parameters:\n",
    "- `catalog`: Dataset catalog returned by `downloader.list_available_datasets()`\n",
    "- `Category/Subcategory`: Choose values to use in Step 7.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135bcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available datasets\n",
    "catalog = downloader.list_available_datasets()\n",
    "\n",
    "print(\"AVAILABLE DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Category':<15} {'Subcategory':<25} {'Description':<30}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for category, subcategories in catalog.items():\n",
    "    for subcategory, info in subcategories.items():\n",
    "        print(f\"{category:<15} {subcategory:<25} {info['description']:<30}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚Üí Copy the category and subcategory values to Step 6.A below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac20d0",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è Step 7: Download Dataset\n",
    "\n",
    "**Download a specific dataset clipped to your catchment extent.**\n",
    "\n",
    "### Step 7.A: Configure Download\n",
    "\n",
    "Specify which dataset to download and clean up any existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c33013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which dataset to download/clean up\n",
    "DATASET_CATEGORY = \"climate\"  # e.g., \"climate\"\n",
    "DATASET_SUBCATEGORY = \"era5_precipitation\"  # e.g., \"era5_precipitation\", \"era5_temperature\"\n",
    "\n",
    "\n",
    "# Build path automatically based on selection\n",
    "dataset_path = build_dataset_path(\n",
    "    PROJECT_BASE, DATASET_CATEGORY, DATASET_SUBCATEGORY, OUTPUT_FORMAT\n",
    ")\n",
    "\n",
    "print(f\"Target dataset: {DATASET_CATEGORY}/{DATASET_SUBCATEGORY}\")\n",
    "print(f\"Path: {dataset_path}\")\n",
    "\n",
    "# Close open datasets, force GC, and remove existing data if present\n",
    "cleanup_existing_dataset(dataset_path, namespace=locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2db8e9",
   "metadata": {},
   "source": [
    "### Step 7.B: Execute Download\n",
    "\n",
    "**Main download operation with automatic retries and verification.**\n",
    "\n",
    "This cell:\n",
    "- Streams data from Azure Blob Storage\n",
    "- Clips spatially to catchment bounds (with buffer)\n",
    "- Applies temporal filtering based on TIME_RANGE\n",
    "- Saves locally in specified format (NetCDF or Zarr)\n",
    "- Verifies downloaded data dimensions and variables\n",
    "- Displays data summary (time range, spatial extent, dimensions)\n",
    "\n",
    "Editable parameters (edit these in Step 7.A or Step 2):\n",
    "- `DATASET_CATEGORY`: Dataset category to download (e.g., `climate`)\n",
    "- `DATASET_SUBCATEGORY`: Dataset subcategory (e.g., `era5_precipitation`)\n",
    "- `TIME_RANGE`: Optional time range tuple `(start_date, end_date)`\n",
    "- `variables`: List of variable names to download (set in code as `None` for all)\n",
    "- `OUTPUT_FORMAT`: Output format set in Step 2 (`nc`, `zarr`, or `dfs2`)\n",
    "\n",
    "**Note:** On Windows, if you get \"file is being used\" errors, run Step 7.A above first to close open datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc542a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "try:\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_path = downloader.download_dataset(\n",
    "        category=DATASET_CATEGORY,\n",
    "        subcategory=DATASET_SUBCATEGORY,\n",
    "        time_range=TIME_RANGE,\n",
    "        variables=None,  # Download all variables\n",
    "    )\n",
    "\n",
    "    download_time = time.time() - start_time\n",
    "\n",
    "    # Verify the download using the same path builder\n",
    "    ds_downloaded = open_dataset_any(output_path)\n",
    "    print(\"Dataset verification:\")\n",
    "    print(f\"  Dimensions: {dict(ds_downloaded.dims)}\")\n",
    "    print(f\"  Variables: {list(ds_downloaded.data_vars)}\")\n",
    "\n",
    "    if \"time\" in ds_downloaded.dims:\n",
    "        print(\n",
    "            f\"  Time range: {str(ds_downloaded.time.min().values)[:10]} to {str(ds_downloaded.time.max().values)[:10]}\"\n",
    "        )\n",
    "\n",
    "    if \"lat\" in ds_downloaded.dims and \"lon\" in ds_downloaded.dims:\n",
    "        if ds_downloaded.dims[\"lat\"] > 0 and ds_downloaded.dims[\"lon\"] > 0:\n",
    "            print(\n",
    "                f\"  Lat range: {float(ds_downloaded.lat.min()):.2f}¬∞ to {float(ds_downloaded.lat.max()):.2f}¬∞\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Lon range: {float(ds_downloaded.lon.min()):.2f}¬∞ to {float(ds_downloaded.lon.max()):.2f}¬∞\"\n",
    "            )\n",
    "\n",
    "    print(f\"\\nDownload completed in {download_time:.1f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Download failed: {e}\")\n",
    "    print(\"Note: Ensure Azure credentials are properly configured\")\n",
    "    print(\"On Windows, if file is locked: Run the cell above first, then retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb7423",
   "metadata": {},
   "source": [
    "### Step 7.C: Batch Download (Optional)\n",
    "\n",
    "**Download all available datasets at once.**\n",
    "\n",
    "Use `download_all()` instead of downloading datasets one at a time. This will iterate through the full catalog and download each dataset clipped to your catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch download all available datasets (optional)\n",
    "# downloader.download_all(time_range=TIME_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec18fbd",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Step 8: Visualize Spatial Data\n",
    "\n",
    "**Create spatial map of downloaded dataset with catchment overlay.**\n",
    "\n",
    "This cell:\n",
    "- Loads the downloaded dataset from local storage\n",
    "- Extracts first variable and first timestep (if temporal)\n",
    "- Loads catchment boundary for overlay\n",
    "- Creates spatial map with catchment outline\n",
    "- Displays variable name, dataset category, and date (if applicable)\n",
    "\n",
    "Editable parameters:\n",
    "- `DATASET_CATEGORY`: Dataset category used to build the path\n",
    "- `DATASET_SUBCATEGORY`: Dataset subcategory used to build the path\n",
    "- `OUTPUT_FORMAT`: Output format used to build the path\n",
    "- `TIMESTEP`: `None`, integer index, or date string for time selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1577fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial data with catchment overlay\n",
    "dataset_path = build_dataset_path(\n",
    "    PROJECT_BASE, DATASET_CATEGORY, DATASET_SUBCATEGORY, OUTPUT_FORMAT\n",
    ")\n",
    "\n",
    "# Configure timestep selection (set to None for first timestep, or specify index/date)\n",
    "TIMESTEP = (\n",
    "    None  # Options: None (first), integer index (e.g., 0, 10, -1), or date string (\"2018-06-15\")\n",
    ")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    ds = open_dataset_any(dataset_path)\n",
    "    print(f\"Loaded: {dict(ds.dims)}, Variables: {list(ds.data_vars)}\")\n",
    "\n",
    "    # Get first variable\n",
    "    var_name = list(ds.data_vars)[0]\n",
    "    data = ds[var_name]\n",
    "\n",
    "    # Select timestep\n",
    "    if \"time\" in data.dims:\n",
    "        if TIMESTEP is None:\n",
    "            data_plot = data.isel(time=0)\n",
    "            time_label = f\" ({str(data.time[0].values)[:10]})\"\n",
    "        elif isinstance(TIMESTEP, int):\n",
    "            data_plot = data.isel(time=TIMESTEP)\n",
    "            time_label = f\" ({str(data.time[TIMESTEP].values)[:10]})\"\n",
    "        elif isinstance(TIMESTEP, str):\n",
    "            data_plot = data.sel(time=TIMESTEP, method=\"nearest\")\n",
    "            time_label = f\" ({str(data_plot.time.values)[:10]})\"\n",
    "        print(\n",
    "            f\"Available time range: {str(data.time.min().values)[:10]} to {str(data.time.max().values)[:10]}\"\n",
    "        )\n",
    "        print(f\"Selected timestep: {time_label.strip(' ()')}\")\n",
    "    else:\n",
    "        data_plot = data\n",
    "        time_label = \"\"\n",
    "\n",
    "    # Use catchment loaded in Step 3\n",
    "    fig, ax = plot_spatial_map(\n",
    "        data_plot,\n",
    "        catchment=catchment,\n",
    "        title=f\"{DATASET_CATEGORY}/{DATASET_SUBCATEGORY}: {var_name}{time_label}\",\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"ERROR: Dataset not found at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa2efb",
   "metadata": {},
   "source": [
    "## üìä Step 9: Time Series Analysis\n",
    "\n",
    "**Compute area-weighted basin average and extract time series.**\n",
    "\n",
    "This cell:\n",
    "- Loads catchment polygon and downloaded dataset\n",
    "- Computes area-weighted intersection fractions for each grid cell\n",
    "- Calculates basin-averaged time series (properly weighted)\n",
    "- Plots time series showing temporal evolution\n",
    "- Computes anomaly statistics (mean, std, percentiles)\n",
    "\n",
    "**Operations:**\n",
    "- Basin averaging accounts for partial grid cell overlap\n",
    "- Time series represents catchment-scale aggregate values\n",
    "- Anomalies show deviations from long-term mean\n",
    "\n",
    "Editable parameters:\n",
    "- `DATASET_CATEGORY`: Dataset category used to build the path\n",
    "- `DATASET_SUBCATEGORY`: Dataset subcategory used to build the path\n",
    "- `OUTPUT_FORMAT`: Output format used to build the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute area-weighted basin time series\n",
    "dataset_path = build_dataset_path(\n",
    "    PROJECT_BASE, DATASET_CATEGORY, DATASET_SUBCATEGORY, OUTPUT_FORMAT\n",
    ")\n",
    "\n",
    "# Use catchment loaded in Step 3\n",
    "ds = open_dataset_any(dataset_path)\n",
    "\n",
    "var_name = list(ds.data_vars)[0]\n",
    "data = ds[var_name]\n",
    "\n",
    "print(f\"Variable: {var_name}, Shape: {dict(ds.dims)}\")\n",
    "\n",
    "# Compute weights and basin average (using catchment_geom from Step 3)\n",
    "weights = compute_catchment_weights(ds, catchment_geom) # type: ignore\n",
    "basin_avg = compute_basin_average(data, weights)\n",
    "\n",
    "# Plot time series\n",
    "if \"time\" in data.dims:\n",
    "    # Compute anomalies first and print stats above the plot\n",
    "    anomaly_stats = compute_anomalies(basin_avg)\n",
    "\n",
    "    fig, ax = plot_time_series(\n",
    "        basin_avg,\n",
    "        label=\"Basin Average (Area-Weighted)\",\n",
    "        title=f\"Area-Weighted Basin {var_name} Time Series\",\n",
    "        ylabel=var_name,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5006b8",
   "metadata": {},
   "source": [
    "## üìù Step 10: Review Download History\n",
    "\n",
    "**Check download log to see what has been downloaded and when.**\n",
    "\n",
    "This cell:\n",
    "- Reads the download log JSON file\n",
    "- Displays download history as formatted table\n",
    "- Shows timestamp, dataset, status (success/error), and file size\n",
    "- Summarizes total downloads and success rate\n",
    "- Useful for tracking data provenance and debugging\n",
    "\n",
    "**Note:** Log file is created after first successful download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check download history\n",
    "log_file = PROJECT_BASE.joinpath(\"logs\", \"download_log.json\")\n",
    "\n",
    "if log_file.exists():\n",
    "    try:\n",
    "        with open(log_file, \"r\") as f:\n",
    "            log_data = json.load(f)\n",
    "\n",
    "        # Handle both dict and list formats\n",
    "        downloads = log_data.get(\"downloads\", []) if isinstance(log_data, dict) else log_data\n",
    "\n",
    "        print(\"Download History:\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, entry in enumerate(downloads, 1):\n",
    "            print(f\"\\n{i}. {entry.get('category', 'N/A')}/{entry.get('subcategory', 'N/A')}\")\n",
    "            print(f\"   Timestamp: {entry.get('timestamp', 'N/A')}\")\n",
    "            print(f\"   Output: {entry.get('output_path', 'N/A')}\")\n",
    "            if entry.get(\"time_range\"):\n",
    "                print(f\"   Time range: {entry['time_range']}\")\n",
    "\n",
    "        print(f\"\\nTotal downloads logged: {len(downloads)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not parse log file: {e}\")\n",
    "else:\n",
    "    print(f\"No download log found at: {log_file}\")\n",
    "    print(\"The log file will be created after the first download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf348fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishes-data-downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
